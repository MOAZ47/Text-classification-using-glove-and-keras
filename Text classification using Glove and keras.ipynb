{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPINION</th>\n",
       "      <th>TAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I recently stayed at the Affina Chicago hotel ...</td>\n",
       "      <td>FAKENEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I stayed at the Affina Chicago for my annivers...</td>\n",
       "      <td>FAKENEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you are looking for a high end hotel on the...</td>\n",
       "      <td>FAKENEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just returned from a long weekend in Chicago...</td>\n",
       "      <td>FAKENEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My wife and I stayed at the Affinia Chicago la...</td>\n",
       "      <td>FAKENEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             OPINION      TAG\n",
       "0  I recently stayed at the Affina Chicago hotel ...  FAKENEG\n",
       "1  I stayed at the Affina Chicago for my annivers...  FAKENEG\n",
       "2  If you are looking for a high end hotel on the...  FAKENEG\n",
       "3  I just returned from a long weekend in Chicago...  FAKENEG\n",
       "4  My wife and I stayed at the Affinia Chicago la...  FAKENEG"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Moaz\\\\Desktop\\\\moaz\\\\WORK\\\\text and sentiment\\\\PEC3\\\\OPINIONS-TAGGED-FAKE.csv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 4 data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FAKENEG', 'FAKEPOS', 'TRUENEG', 'TRUEPOS'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TAG'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have extracted data from dataframe to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data_labels = []\n",
    "\n",
    "opinions = df['OPINION'].tolist()\n",
    "tags = df['TAG'].tolist()\n",
    "\n",
    "for i in range(len(opinions)): \n",
    "    data.append(opinions[i]) \n",
    "    data_labels.append(tags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have encoded data label into label encoded values and one hot encoded values. I will be using integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAKENEG' 'FAKENEG' 'FAKENEG' ... 'TRUEPOS' 'TRUEPOS' 'TRUEPOS']\n",
      "------------------------------------\n",
      "[0 0 0 ... 3 3 3]\n",
      "------------------------------------\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "values = array(data_labels)\n",
    "print(values)\n",
    "print('------------------------------------')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "print('------------------------------------')\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create embedding I have used glove.42B.300d, from Glove. I have also padded the data to make them of equal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9838\n",
      "300\n",
      "Padded length: 1600\n",
      "Split: 1280\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size=len(word_index)\n",
    "print('Vocab size:',vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "embeddings_index = {};\n",
    "with open('C:\\\\Users\\\\Moaz\\\\Desktop\\\\GLove\\\\glove.42B.300d.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32');\n",
    "        embeddings_index[word] = coefs;\n",
    "print(len(coefs))\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size+1, 300));\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word);\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector;\n",
    "\n",
    "\n",
    "        \n",
    "# Padding data\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "padded = pad_sequences(sequences, maxlen=500, padding='post', truncating='post')\n",
    "print('Padded length:',len(padded))\n",
    "split = 0.2\n",
    "split_n = int(round(len(padded)*(1-split),0))\n",
    "print('Split:',split_n)\n",
    "\n",
    "train_data = padded[:split_n]\n",
    "train_labels = integer_encoded[:split_n]\n",
    "test_data = padded[split_n:]\n",
    "test_labels = integer_encoded[split_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  1600\n",
      "Padded length: (1600, 500)\n",
      "Train Data:  (1280, 500)\n",
      "Train label:  (1280, 1)\n",
      "Test Data:  (320, 500)\n",
      "Test label:  (320, 1)\n",
      "Embedding matrix:  9839\n"
     ]
    }
   ],
   "source": [
    "print('Data: ',len(data))\n",
    "print('Padded length:',padded.shape)\n",
    "print('Train Data: ',train_data.shape)\n",
    "print('Train label: ',train_labels.shape)\n",
    "print('Test Data: ',test_data.shape)\n",
    "print('Test label: ',test_labels.shape)\n",
    "print('Embedding matrix: ',len(embeddings_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 500, 300)          2951700   \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 1204      \n",
      "=================================================================\n",
      "Total params: 3,674,104\n",
      "Trainable params: 722,404\n",
      "Non-trainable params: 2,951,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1024 samples, validate on 256 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 66s 65ms/step - loss: 1.2799 - val_loss: 3.0513\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.05133, saving model to model.h1.14_jun_20\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 82s 80ms/step - loss: 1.0998 - val_loss: 3.3820\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.05133\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 84s 82ms/step - loss: 1.0914 - val_loss: 3.2069\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.05133\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 79s 78ms/step - loss: 1.0820 - val_loss: 3.2411\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.05133\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 90s 88ms/step - loss: 1.0780 - val_loss: 3.1666\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.05133\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 85s 83ms/step - loss: 1.0631 - val_loss: 3.2524\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.05133\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 85s 83ms/step - loss: 1.0640 - val_loss: 3.0516\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.05133\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 85s 83ms/step - loss: 1.0664 - val_loss: 3.2482\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.05133\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 86s 84ms/step - loss: 1.0670 - val_loss: 3.1272\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.05133\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 86s 84ms/step - loss: 1.0644 - val_loss: 3.2178\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.05133\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÃ“N                                  #\n",
    "#############################################\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from keras.layers import Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 300, weights = [embeddings_matrix], input_length=500, trainable=False))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "filename = 'model.h1.14_jun_20'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, batch_size=100,\n",
    "                    validation_split = 0.2, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "model = load_model('model.h1.14_jun_20')\n",
    "preds = model.predict_classes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.051326</td>\n",
       "      <td>1.279903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.381990</td>\n",
       "      <td>1.099803</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.206869</td>\n",
       "      <td>1.091420</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.241063</td>\n",
       "      <td>1.081965</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.166579</td>\n",
       "      <td>1.078018</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   val_loss      loss  epoch\n",
       "0  3.051326  1.279903      0\n",
       "1  3.381990  1.099803      1\n",
       "2  3.206869  1.091420      2\n",
       "3  3.241063  1.081965      3\n",
       "4  3.166579  1.078018      4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
